<!DOCTYPE html>
<html lang="en" data-theme="light">
  <head>
    <meta charset="utf-8">
    <meta content="ie=edge" http-equiv="x-ua-compatible">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

    <!-- Highlight.js Tokyo Night (dark) from jsDelivr / highlight.js CDN -->
<link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.11.1/build/styles/tokyo-night-dark.min.css">


    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SLLMJ5YC4K"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SLLMJ5YC4K');
</script>

  </head>

  <body class="bg-slate-50 text-base-content min-h-screen p-6">
    <div class="flex justify-between items-center w-full">
  <a href="http://localhost:4000" class="flex-1">
    <img class="w-12 h-12 rounded-full" src="/assets/images/headshot.jpg" />
  </a>

  <div class="hidden sm:flex w-fit justify-center items-center bg-slate-200 p-2 rounded-full font-semibold">
    <a href="/" class="hover:bg-slate-300 py-2 px-4 rounded-full">Home</a>
    <a href="https://drive.google.com/file/d/10v_i5GDWsbjHuOF4qq_nvTrEcAoZPUfY/view?usp=sharing" target="_blank" class="hover:bg-slate-300 py-2 px-4 rounded-full">Resume</a>
    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')" class="hover:bg-slate-300 py-2 px-4 rounded-full">Contact</a>
  </div>

  <div class="hidden sm:flex gap-4 flex-1 justify-end">
    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')">
      <i class="fa-solid fa-xl fa-envelope"></i>
    </a>


    <a href="http://localhost:4000/feed.xml">
      <i class="fa-solid fa-xl fa-rss"></i>
    </a>
  </div>

  <label class="block sm:hidden relative z-40 cursor-pointer px-3 py-6 flex-1 flex justify-end" for="mobile-menu">
    <input class="peer hidden" type="checkbox" id="mobile-menu" />
    <div
        class="relative z-50 block h-[2px] w-7 bg-black bg-transparent content-[''] before:absolute before:top-[-0.35rem] before:z-50 before:block before:h-full before:w-full before:bg-black before:transition-all before:duration-200 before:ease-out before:content-[''] after:absolute after:right-0 after:bottom-[-0.35rem] after:block after:h-full after:w-full after:bg-black after:transition-all after:duration-200 after:ease-out after:content-[''] peer-checked:bg-transparent before:peer-checked:top-0 before:peer-checked:w-full before:peer-checked:rotate-45 before:peer-checked:transform after:peer-checked:bottom-0 after:peer-checked:w-full after:peer-checked:-rotate-45 after:peer-checked:transform"
        >
    </div>
      <div
          class="fixed inset-0 z-40 hidden h-full w-full bg-black/50 backdrop-blur-sm peer-checked:block"
          >
          &nbsp;
      </div>
        <div
            class="fixed top-0 right-0 z-40 h-full w-full translate-x-full overflow-y-auto overscroll-y-none transition duration-500 peer-checked:translate-x-0"
            >
            <div class="float-right min-h-full w-[80%] bg-slate-50 px-8 pt-24 shadow-2xl">
              <menu class="flex flex-col gap-4 text-xl">
                <li><a href="/" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Home</a></li>
                <li><a href="https://drive.google.com/file/d/10v_i5GDWsbjHuOF4qq_nvTrEcAoZPUfY/view?usp=sharing" target="_blank" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Resume</a></li>
                <li><a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Contact</a></li>
              </menu>
            </div>
        </div>
  </label>
</div>


    <main class="container mx-auto">
      <div class="max-w-screen-md mx-auto mt-24">
  <div class="flex flex-col gap-2 sm:mt-12 mb-12">
    <h1 class="text-6xl text-primary tracking-tight mb-4">Deep Q-Learning for Atari Games</h1>
    <time class="">01 Jan 2019</time>
  </div>

  <article class="mx-auto prose lg:prose-xl text-base-content">
    <p><img src="https://upload.wikimedia.org/wikipedia/en/thumb/f/f8/Etvideogamecover.jpg/220px-Etvideogamecover.jpg" alt="The worst game ever made" /></p>

<p>Over the last few posts we introduced the topic of Q-Learning and Deep Q-Learning in the field of reinforcement learning. We looked at how we can use the Bellman Equation to calculate the quality of taking a particular action at a given state. We originally used a Q-table to keep track of our state action pairs and eventually replaced it with  a neural network to handle a larger state space by approximating our Q-values, rather than storing them for every possible state action pair.</p>

<p>We‚Äôll improve on our last tutorial of building a deep Q-network for the CartPole game, by throwing in a preprocessing step that allows us to learn from image data, rather than just the handy values we get back from OpenAI‚Äôs gym library. We‚Äôve covered convolutional neural nets before, but if you‚Äôre not familiar, I would recommend brushing up on them first, as well as the past two posts on Q-Learning and Deep Q-Learning.</p>

<p>In this post, we‚Äôll combine deep Q-learning with convolutional neural nets, to build an agent that learns to play Space Invaders. In fact, our agent can learn to play a wide variety of Atari games, so feel free to swap out Space Invaders for any game listed here: <a href="https://gym.openai.com/envs/#atari">https://gym.openai.com/envs/#atari</a></p>

<h2 id="lets-recap">Let‚Äôs recap</h2>

<p>The bellman equation let‚Äôs us assess the q-value (quality) of a given state action pair. It states that the quality of taking an action at a given state, is equal to the immediate reward, plus the maximum discounted reward of the next state.</p>

<h3 id="qs-a--r--Œ≥-max‚Çêqs-a">Q(s, a) = r + Œ≥ max‚Çê‚Äô(Q(s‚Äô, a‚Äô))</h3>

<p>In other words, we‚Äôll use a neural network to predict what action gives us the biggest future reward at any given state, by not only looking at the immediate state, but also, our prediction for the one that comes after it.</p>

<p>Initially, we know nothing about our game environment, so we need to explore it by making random moves and observing the outcome. After a while, we‚Äôll start slowly moving away from this exploration approach and into an approach of exploiting our predictions, in order to improve them and win the game.</p>

<p>If we exploit too early, we won‚Äôt get chance to try new novel ideas which could improve our performance. If we explore too much, we won‚Äôt make progress. This is known as the exploration vs exploitation tradeoff.</p>

<h2 id="experience-replay">Experience Replay</h2>

<p>In our last post we introduced the concept of experience replay. Experience replay helps our network to learn from past actions. At each step, we‚Äôll take our observation and append it to the end of a list (which we‚Äôll call our ‚Äòmemory‚Äô). We implement the list as a deque in python, a double ended queue of fixed size that automatically removes the oldest element every time we add something new to the list. We‚Äôll then feed this minibatch into our network to train our predictions of Q values. As our network improves, so do our experiences, which feeds back into our network.</p>

<p>Last time we used a relatively short memory, but this time, we‚Äôre going to store the last one million frames of gameplay.</p>

<h2 id="convnet">Convnet</h2>

<p>We‚Äôll swap out our standard neural network for a convolutional neural network and learn to make decisions based on nothing but the raw pixel data of our game. This means that our agent will have to learn what is an enemy, what is a ball, what shooting does, and all other possible actions and consequences. The advantage of this is that we‚Äôre no longer tied to a game. Our agent will be able to learn a wide variety of Atari games based purely on pixel input.</p>

<p>Our convnet architecture if pretty standard, we‚Äôll have three convolutional layers, a flatten layer, and two fully connected layers. The only difference is the we‚Äôll omit the max pooling layers.</p>

<p>Max Pooling aims to make our network insensitive to small changes in the positions of features within our image. As our agent needs to know exactly where things are in our game, we‚Äôll get rid of the traditional max pooling layers in our convnet all together.</p>

<h2 id="stacked-frames">Stacked Frames</h2>

<p>When we feed our frames into our convnet, we‚Äôll actually use a stack of 4 frames. If you think about a single frame of a game of Pong, it‚Äôs impossible to know the direction the ball is going in or how fast. Using a stack of four frames gives us a sense of motion and speed that is necessary for our network to have the full picture. You can think of it like a mini video clip being fed to our network. Instead of our input being a single frame of the shape (105,80,1), we‚Äôll now have four channels, taking the shape to (105,80,4).</p>

<h2 id="frame-skipping">Frame Skipping</h2>

<p>In their original paper, DeepMind skipped four frames every time they looped through gameplay. Their reasoning for doing this was that the environment doesn‚Äôt change much between single frames, we‚Äôd get a better representation of speed and movement by only looking at every fourth frame, plus we would reduce the amount of frame we need to process.</p>

<p>We‚Äôll use frame skipping in our implementation, but how do we implement it? Fortunately this has been taken care of in OpenAI‚Äôs gym library.</p>

<hr />

<blockquote>
  <p><em>Maximize your score in the Atari 2600 game MsPacman. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3) Each action is repeatedly performed for a duration of kkk frames, where kkk is uniformly sampled from {2,3,4}{2, 3, 4}{2,3,4}.</em></p>
</blockquote>

<hr />

<p>The version number at the end of most games (<code>gym.make('MsPacman-v4')</code>) isn‚Äôt a version number at all, but refers to the amount of frames we skip. We can skip anywhere from no frames, to four frames by amending the number at the end of our environment name. For example‚Ä¶</p>

<ul>
  <li>MsPacman-v0 = No frame skipping</li>
  <li>MsPacman-v2 = Look at every second frame</li>
  <li>MsPacman-v3 = Look at every third frame</li>
  <li>MsPacman-v4 = Look at every fourth frame</li>
</ul>

<h2 id="performance">Performance</h2>

<p>Storing a million frames of pixels in memory can be quite computationally expensive. Our arrays for a single frame are 105 by 80 pixels, that‚Äôs 8400 pixels per frame. The numpy default array stores each of these pixels as a 32 bit float, meaning that our total memory (8400 * 32bits * 1000000) could take up to 33.6 gigabytes of RAM!</p>

<p>To combat this, we‚Äôll specify our datatypes as uint8 for our frames and convert them to floats at the last minute before we feed them into our network. This will bring our RAM usage down from 33.6 to 8.4 gigabytes, much better!</p>

<h2 id="building-our-dqn">Building our DQN</h2>

<p>Let‚Äôs start by importing our dependencies‚Ä¶</p>

<pre><code class="language-python">import numpy as np
from keras import Sequential
from keras.layers import Dense, Flatten
from keras.layers.convolutional import Conv2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from collections import deque
import gym
import random
</code></pre>

<p>Next we‚Äôll define our DQNetwork class. I‚Äôll keep indentation consistent, but I‚Äôll break up some of the code so that we can walk through it block by block and really understand what‚Äôs happening.</p>

<pre><code class="language-python">class DQNetwork:
    def __init__(self, env):
        self.env              = env
        self.state_size       = env.observation_space.shape[0]
        self.action_size      = env.action_space.n
        self.memory           = deque(maxlen=1000000)
        self.stack            = deque([np.zeros((105,80), dtype=np.uint8) for i in range(4)], maxlen=4)
        self.gamma            = 0.9
        self.epsilon          = 1.0
        self.epsilon_min      = 0.01
        self.epsilon_decay    = 0.00003
        self.learning_rate    = 0.00025
        self.batch_size       = 64
        self.frame_size       = (105, 80)
        self.possible_actions = np.array(np.identity(self.action_size, dtype=int).tolist())
        self.model            = self.build_model()
</code></pre>

<p>Our <code>__init__</code> method is mostly the same as our last model. We‚Äôre setting up some key parameters to use later, like our gamma, epsilon (exploration vs exploitation trade off), our deque for our memory, and building and storing our model.</p>

<p>The new things are‚Ä¶</p>

<ul>
  <li><code>stack</code> - A smaller deque to help stack our four frames together to show our network a sense of motion</li>
  <li><code>possible_actions</code> - One hot encoded list of our possible actions (will come in handy later)</li>
  <li><code>frame_size</code> - The size of our preprocessed frames. It makes sense to abstract this out as we‚Äôll be typing this a lot</li>
</ul>

<p>Next we‚Äôll need to think about preprocessing our frames before feeding them into our network. We‚Äôll greyscale them as colour doesn‚Äôt add any additional information to our network and would take up three times the space (red, green and blue channels as opposed to a single greyscale channel). Notice we‚Äôre storing our frames as uint8 and not normalizing our frames to be between 0-1 (which we would traditionally do to prepare our data for our network). Instead, we‚Äôll normalize on demand later on to save memory.</p>

<pre><code class="language-python">    def preprocess_frame(self, frame):
        """Resize frame and greyscale, store as uint8 and normalize on demand to save memory"""
        frame = frame[::2, ::2]
        return np.mean(frame, axis=2).astype(np.uint8)
</code></pre>

<p>We‚Äôll also need a method to append a frame to the end of our four frame stack deque that we defined earlier. Our deque will handle removing the oldest frame, but there is an exception that we need to handle. At the beginning of our game, we‚Äôll need to stack the same frame four times to fill out our stack. We‚Äôll have our method take an optional <code>reset=True</code> parameter that clears the stack and adds the same frame four times. Our final stacked state that we pass into our network will end up being of the shape (105,80,4).</p>

<pre><code class="language-python">    def append_to_stack(self, state, reset=False):
        """Preprocesses a frame and adds it to the stack"""
        frame = self.preprocess_frame(state)

        if reset:
            # Reset stack
            self.stack = deque([np.zeros((105,80), dtype=np.int) for i in range(4)], maxlen=4)

            # Because we're in a new episode, copy the same frame 4x
            for i in range(4):
                self.stack.append(frame)
        else:
            self.stack.append(frame)

        # Build the stacked state (first dimension specifies different frames)
        stacked_state = np.stack(self.stack, axis=2)

        return stacked_state
</code></pre>

<p>We‚Äôll need to create a similar method to store our experiences in memory, and retrieve a random minibatch‚Ä¶</p>

<pre><code class="language-python">    def remember(self, state, action, reward, new_state, done):
        self.memory.append((state, action, reward, new_state, done))

    def memory_sample(self, batch_size):
        """Sample a random batch of experiences from memory"""
        memory_size = len(self.memory)
        index       = np.random.choice(np.arange(memory_size), size=batch_size, replace=False)
        return [self.memory[i] for i in index]
</code></pre>

<p>Next, we‚Äôll build our model. This is almost identical as last time, except that we‚Äôre using the Conv2D layer from Keras, and exclusing the traditional max pooling layer that we‚Äôd normally add with a convolutional network. (Remember, max pooling makes our network insensitive to position changes. Great for object detection and classification, but not great when our game depends on the position of the features we detect!)</p>

<pre><code class="language-python">    def build_model(self):
        """Build the neural net model"""
        model = Sequential()
        model.add(Conv2D(32, (8, 4), activation='elu', input_shape=(105, 80, 4)))
        model.add(Conv2D(64, (3, 2), activation='elu'))
        model.add(Conv2D(64, (3, 2), activation='elu'))
        model.add(Flatten())
        model.add(Dense(512, activation='elu', kernel_initializer='glorot_uniform'))
        model.add(Dense(self.action_size, activation='softmax'))
        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))
        return model
</code></pre>

<p>Our agent will need to be able to make two types of move, depending on where we are in our exploration vs exploitation journey. We‚Äôll write a method that returns a random action, and a method which takes in our state (105,80,4), and predicts the best action (according to our neural network).</p>

<p>Notice in the <code>predict_action</code> method, we first divide by 255 to normalize our values between 0 and 1. Secondly, we‚Äôll reshape our state from (105,80,4), to (1,105,80,4), a necessary step for keras to consume our data. You can think of our shape like this: (number of examples, height, width, depth). Our network will return a vector the size of our possible actions, from which, we‚Äôll return the index of the action we predicted, ready to feed into our <code>env.step</code> call.</p>

<pre><code class="language-python">    def random_action(self):
        """Returns a random action"""
        return random.randint(1,len(self.possible_actions)) - 1

    def predict_action(self, state):
        """Returns index of best predicted action"""
        state  = state / 255
        state  = state.reshape((1, *state.shape)) # Reshape our state to a single example for our neural net
        choice = self.model.predict(state)
        return np.argmax(choice)
</code></pre>

<p>With our <code>random_action</code> and <code>predict_action</code> methods defined, we can now write a function to select which one to choose depending on where we are on our explore vs exploit spectrum.</p>

<p>We‚Äôll also use a slightly different way of calculating our explore vs exploit probability depending on the step in our game play. Lastly, we‚Äôll return our <code>explore_probability</code> to log out later.</p>

<pre><code class="language-python">    def select_action(self, state, decay_step):
        """Returns an action to take with decaying exploration/exploitation"""

        explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)

        if explore_probability &gt; np.random.rand():
            # Exploration
            return self.random_action(), explore_probability
        else:
             # Exploitation
            return self.predict_action(state), explore_probability
</code></pre>

<h2 id="training">Training</h2>

<p>With the majority of our agent built, there‚Äôs only one more method to implement; training our model with experiences from our replay memory.</p>

<p>Firstly, we‚Äôll check to see if our memory is less than our batch size of 64. If we don‚Äôt have enough experiences logged yet, we‚Äôll exit the function and let our agent keep gathering random experiences until we have enough experience to form a complete minibatch to train on.</p>

<p>Next we prepare our minibatch. First we‚Äôll select a random minibatch of 64 experiences, notice we also divide our <code>states_mb</code> and <code>next_states_mb</code> by 255 to normalise our frames to be between 0 and 1. Next, we‚Äôll grab our predictions for our current state (shape (64, 105, 80, 4)), as well as the predictions for our next states.</p>

<p>With our predictions, we can assemble a corresponding list of the Q-values for each state. If we‚Äôve reached a terminal state, and the game is over, then our Q-value is equal to the final reward (as there are no more future rewards). If we‚Äôve not yet reached the end of our game, then our Q-value is set to the immediate reward (from the <code>rewards_mb[i]</code> list, plus the maximum discounted future reward (gamma * the maximum reward from our next state prediction).</p>

<p>Once we‚Äôve finished our corresponding Q-values list, we can fit our model for one epoch, with our <code>states_mb</code> as our input, and our <code>targets_mb</code> as our labels. A single iteration doesn‚Äôt seem much here, but remember we‚Äôll be calling this replay method at every step throughout our gameplay.</p>

<pre><code class="language-python">   def replay(self):
        if len(self.memory) &lt; self.batch_size:
            return

        # Select a random minibatch from memory
        minibatch = self.memory_sample(self.batch_size)

        # Split out our tuple and normalise our states
        states_mb      = np.array([each[0] for each in minibatch]) / 255
        actions_mb     = np.array([each[1] for each in minibatch])
        rewards_mb     = np.array([each[2] for each in minibatch])
        next_states_mb = np.array([each[3] for each in minibatch]) / 255
        dones_mb       = np.array([each[4] for each in minibatch])

        # Get our predictions for our states and our next states
        target_qs         = self.model.predict(states_mb)
        predicted_next_qs = self.model.predict(next_states_mb)

        # Create an empty targets list to hold our Q-values
        target_Qs_batch = []

        for i in range(0, len(minibatch)):
            done = dones_mb[i]

            if done:
                # If we finished the game, our q value is the final reward (as there are no more future rewards)
                q_value = rewards_mb[i]
            else:
                # If we havent, our q value is the immediate reward, plus future discounted reward (gamma is our discount)
                q_value = rewards_mb[i] + self.gamma * np.max(predicted_next_qs[i])

            # Fit target to a vector for keras (represent actions as one hot * q value (q gets set at the action we took, everything else is 0))

            one_hot_target = self.possible_actions[actions_mb[i]]
            target         = one_hot_target * q_value
            target_Qs_batch.append(target)

        targets_mb = np.array([each for each in target_Qs_batch])

        self.model.fit(states_mb, targets_mb, epochs=1, verbose=1) # Change to verbose=0 to disable logging
</code></pre>

<h2 id="training-our-dqn">Training our DQN</h2>

<p>With our DQNetwork class complete, we just need to train our model. As we‚Äôve dramatically increased our state space, our model is going to take quite a long time to train. We‚Äôre training for around 2.5 million frames of game play (50 episodes, each with a maximum of 50,000 steps per game), a conventional laptop isn‚Äôt going to cut it here (unless you‚Äôve got a lot of RAM and are happy to leave it running for a week or two!).</p>

<p>I‚Äôve included a section about my recommendations for training on an AWS instance below. But first, let‚Äôs talk about what‚Äôs happening in our training loop.</p>

<p>We‚Äôll start by initialising our environment, as well as a monitor wrapper which will record each episode to video for us to review later. We‚Äôll loop through our episodes, taking a maximum of 50000 steps per game.</p>

<p>At each step, we‚Äôll pick an action based on exploration/exploitation and observe the reward and new state. We‚Äôll append these to our memory, as we‚Äôll need them to train on later.</p>

<p>If it turns out we‚Äôve finished our game and are at the terminal state, we‚Äôll create a blank frame to represent our <code>next_state</code> add to our stack. This let‚Äôs us record the final reward, if we didn‚Äôt stack a blank frame, we‚Äôd lose all the information and rewards we were awarded at the final state.</p>

<p>If we‚Äôre still playing our game, we‚Äôll add our frame to the end of our four frame stack, set the <code>state</code> equal to the <code>next_state</code> to move the game on, and train our agent on a random minibatch of 64 previous experiences.</p>

<pre><code class="language-python">env         = gym.make('Pong-v4')
env         = gym.wrappers.Monitor(env, './videos/', video_callable=lambda episode_id: True) # Save each episode to video
agent       = DQNetwork(env)
episodes    = 50
steps       = 50000
decay_step  = 0

for episode in range(episodes):
    episode_rewards = []

    # 1. Reset the env and frame stack
    state         = agent.env.reset()
    state         = agent.append_to_stack(state, reset=True)

    for step in range(steps):
        decay_step += 1

        # 2. Select an action to take based on exploration/exploitation
        action, explore_probability = agent.select_action(state, decay_step)

        # 3. Take the action and observe the new state
        next_state, reward, done, info = agent.env.step(action)

        # Store the reward for this move in the episode
        episode_rewards.append(reward)

        # 4. If game finished...
        if done:
            # Create a blank next state so that we can save the final rewards
            next_state = np.zeros((210,160,3), dtype=np.uint8)
            next_state = agent.append_to_stack(next_state)

            # Add our experience to memory
            agent.remember(state, action, reward, next_state, done)

            # Save our model
            agent.model.save_weights("model-ep-{}.h5".format(episode))

            # Print logging info
            print("Game ended at episode {}/{}, total rewards: {}, explore_prob: {}".format(episode, episodes, np.sum(episode_rewards), explore_probability))
            # Start a new episode
            break
        else:
            # Add the next state to the stack
            next_state = agent.append_to_stack(next_state)

            # Add our experience to memory
            agent.remember(state, action, reward, next_state, done)

            # Set state to the next state
            state = next_state

        # 5. Train with replay
        agent.replay()
</code></pre>

<h2 id="training-on-ec2">Training on EC2</h2>

<p>I opted to train my model using a p2.xlarge instance on EC2. I ran the code as a regular python file, within a tmux session. That way I could detatch from the session and it would keep running. If you were to try running this inside a Jupyter notebook, the code would stop running as soon as you closed your browser or laptop, given that this can take days or weeks to train, it‚Äôs best to have an environment you can completely detatch from and come back to later.</p>

<p>You can follow this tutorial to get Jupyter Notebook up and running on an EC2 instance with GPU (follow up to the jupyter part to get your EC2 instance running):</p>

<p><a href="https://medium.com/@margaretmz/setting-up-aws-ec2-for-running-jupyter-notebook-on-gpu-c281231fad3f">https://medium.com/@margaretmz/setting-up-aws-ec2-for-running-jupyter-notebook-on-gpu-c281231fad3f</a></p>

<p>Once you‚Äôve set up your EC2 instance, you‚Äôll need to ssh into your instance, install some dependencies and download the roms for the Atari games‚Ä¶</p>

<pre><code>sudo apt install unrar
sudo apt install ffmpeg
</code></pre>

<p>Download and import Atari roms‚Ä¶</p>

<pre><code>wget http://www.atarimania.com/roms/Roms.rar
unrar x Roms.rar &amp;&amp; unzip Roms/ROMS.zip
pip install gym gym-retro gym[atari]
python -m retro.import ROMS/
</code></pre>

<h2 id="results">Results</h2>

<p>Here‚Äôs Playing at episode 1. Some times we‚Äôll hit the ball accidentally, but we‚Äôre still in the explore phase, so a lot of our movement is random and jittery.</p>

<p><img src="/assets/images/deep_q_learning_for_atari_games/pong_ep_1.gif" alt="Pong at episode one" /></p>

<p>Updates coming over the next few days as training completes!</p>

<h2 id="resources">Resources</h2>

<p>Here are a couple of articles that really helped me with wrapping my head around the implementation of this‚Ä¶</p>

<p><a href="https://ai.intel.com/demystifying-deep-reinforcement-learning/#gs.AfY3CNJe">https://ai.intel.com/demystifying-deep-reinforcement-learning/#gs.AfY3CNJe</a></p>

<p><a href="https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26">https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26</a></p>

<p><a href="https://medium.com/@margaretmz/setting-up-aws-ec2-for-running-jupyter-notebook-on-gpu-c281231fad3f">https://medium.com/@margaretmz/setting-up-aws-ec2-for-running-jupyter-notebook-on-gpu-c281231fad3f</a></p>

  </article>

  <div role="alert" class="mt-24 mb-12 alert alert-error alert-dash alert-vertical sm:alert-horizontal">
  üëç
  <div>
    <h3 class="font-bold">Thanks for reading</h3>
    <p class="">You can follow me here for my latest thoughts and projects</p>
  </div>
  <div class="flex items-center gap-4">
    <a href="https://github.com/spencerldixon" target="_blank">
      <i class="fa-brands fa-github fa-xl"></i>
    </a>

    <a href="https://twitter.com/spencerldixon" target="_blank">
      <i class="fa-brands fa-x-twitter fa-xl"></i>
    </a>

    <a href="https://linkedin.com/in/spencerldixon" target="_blank">
      <i class="fa-brands fa-linkedin fa-xl"></i>
    </a>

    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')">
      <i class="fa-solid fa-envelope fa-xl"></i>
    </a>

    <a href="http://localhost:4000/feed.xml">
      <i class="fa-solid fa-rss fa-xl"></i>
    </a>
  </div>
</div>




</div>

    </main>

    <!-- Highlight.js core -->
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.11.1/build/highlight.min.js"></script>
    <script>
    // Auto-detect & highlight all <pre><code class="language-..."> blocks
    document.addEventListener("DOMContentLoaded", (event) => {
      if (window.hljs) { hljs.highlightAll(); }
    });
    </script>
  </body>
</html>
