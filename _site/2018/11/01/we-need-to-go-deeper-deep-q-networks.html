<!DOCTYPE html>
<html lang="en" data-theme="light">
  <head>
    <meta charset="utf-8">
    <meta content="ie=edge" http-equiv="x-ua-compatible">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

    <!-- Highlight.js Tokyo Night (dark) from jsDelivr / highlight.js CDN -->
<link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.11.1/build/styles/tokyo-night-dark.min.css">


    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SLLMJ5YC4K"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SLLMJ5YC4K');
</script>

  </head>

  <body class="bg-slate-50 text-base-content min-h-screen p-6">
    <div class="flex justify-between items-center w-full">
  <a href="http://localhost:4000" class="flex-1">
    <img class="w-12 h-12 rounded-full" src="/assets/images/headshot.jpg" />
  </a>

  <div class="hidden sm:flex w-fit justify-center items-center bg-slate-200 p-2 rounded-full font-semibold">
    <a href="/" class="hover:bg-slate-300 py-2 px-4 rounded-full">Home</a>
    <a href="https://drive.google.com/file/d/10v_i5GDWsbjHuOF4qq_nvTrEcAoZPUfY/view?usp=sharing" target="_blank" class="hover:bg-slate-300 py-2 px-4 rounded-full">Resume</a>
    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')" class="hover:bg-slate-300 py-2 px-4 rounded-full">Contact</a>
  </div>

  <div class="hidden sm:flex gap-4 flex-1 justify-end">
    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')">
      <i class="fa-solid fa-xl fa-envelope"></i>
    </a>


    <a href="http://localhost:4000/feed.xml">
      <i class="fa-solid fa-xl fa-rss"></i>
    </a>
  </div>

  <label class="block sm:hidden relative z-40 cursor-pointer px-3 py-6 flex-1 flex justify-end" for="mobile-menu">
    <input class="peer hidden" type="checkbox" id="mobile-menu" />
    <div
        class="relative z-50 block h-[2px] w-7 bg-black bg-transparent content-[''] before:absolute before:top-[-0.35rem] before:z-50 before:block before:h-full before:w-full before:bg-black before:transition-all before:duration-200 before:ease-out before:content-[''] after:absolute after:right-0 after:bottom-[-0.35rem] after:block after:h-full after:w-full after:bg-black after:transition-all after:duration-200 after:ease-out after:content-[''] peer-checked:bg-transparent before:peer-checked:top-0 before:peer-checked:w-full before:peer-checked:rotate-45 before:peer-checked:transform after:peer-checked:bottom-0 after:peer-checked:w-full after:peer-checked:-rotate-45 after:peer-checked:transform"
        >
    </div>
      <div
          class="fixed inset-0 z-40 hidden h-full w-full bg-black/50 backdrop-blur-sm peer-checked:block"
          >
          &nbsp;
      </div>
        <div
            class="fixed top-0 right-0 z-40 h-full w-full translate-x-full overflow-y-auto overscroll-y-none transition duration-500 peer-checked:translate-x-0"
            >
            <div class="float-right min-h-full w-[80%] bg-slate-50 px-8 pt-24 shadow-2xl">
              <menu class="flex flex-col gap-4 text-xl">
                <li><a href="/" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Home</a></li>
                <li><a href="https://drive.google.com/file/d/10v_i5GDWsbjHuOF4qq_nvTrEcAoZPUfY/view?usp=sharing" target="_blank" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Resume</a></li>
                <li><a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Contact</a></li>
              </menu>
            </div>
        </div>
  </label>
</div>


    <main class="container mx-auto">
      <div class="max-w-screen-md mx-auto mt-24">
  <div class="flex flex-col gap-2 sm:mt-12 mb-12">
    <h1 class="text-6xl text-primary tracking-tight mb-4">We need to go deeper: Deep Q Networks</h1>
    <time class="">01 Nov 2018</time>
  </div>

  <article class="mx-auto prose lg:prose-xl text-base-content">
    <p>In the last post we looked at Q-Learning with respect to reinforcement learning; the idea that we can assess the quality of a particular state action pair and build up a cheat sheet that allows us to play the game proficiently.</p>

<p>Unfortunately we quickly came to the bottleneck in this problem; that as our state space grows, it becomes more and more computationally expensive to calculate the quality for every possible state action pair (that coupled with the fact that this only works on an environment that can be modelled with a Markov Decision Process). Instead of creating a Q-table (check out the previous post if you‚Äôre not familiar with Q-tables), we need a way to approximate the quality of an action without storing every possible combination of state action pairs.</p>

<p>Enter the good old neural network.</p>

<p>A neural network works like a blank brain that you can train to associate some input with some output. Give it 10,000 images of cats and dogs, along with the correct answers, and it will map the input to the output and be able to classify cat or dog on a new image that it hasn‚Äôt seen before. The caveat here is that you need to provide the correct answers during training. This means neural networks are a <em>supervised</em> learning problem.</p>

<p>Mathematically, we‚Äôre simply seeking to minimise the difference between the predictions from our neural net, and the actual correct answers. As long as there is a small error, our neural net will, on average, predict the same thing as the correct answer.</p>

<p>Enter our game for this post and our loss function‚Ä¶</p>

<h2 id="learning-to-play-cartpole">Learning to play CartPole</h2>

<p>Cart Pole is a game which ships with OpenAI‚Äôs gym library for reinforcement learning. It consists of a pole, hinged on a movable cart. The objective is simple; move the cart left or right to keep the pole balanced and upright.</p>

<p><img src="/assets/images/we_need_to_go_deeper/cartpole.gif" alt="Cartpole" /></p>

<p>But there‚Äôs a problem. With reinforcement learning, we seek to maximise our cumulative rewards over time. If we received a reward for moving the cart to the right to retain balance of the pole, then we may try moving the cart right again to get another reward. This unwanted behaviour is rampant in reinforcement learning and demonstrates how a simple oversight can turn good AI bad.</p>

<p>Instead of maximising reward, we want to maximise time. Our agent‚Äôs goal will be to keep the game going for as long as possible.</p>

<h2 id="experience-replay">Experience Replay</h2>

<p>Imagine we‚Äôre playing a game where our enemy pops out at either the right, or left of the screen. Each round is random, but suppose we get a large amount of rounds that favour one particular side. As our agent is trained sequentially, our neural net begins to favour that particular side and develops a bias in its prediction of future actions. In other words, we start to favour recent data and forget past experiences.</p>

<p>How do we train our neural net in a way that it doesn‚Äôt favour what it‚Äôs recently learned? How do we prevent our neural net from forgetting past experiences that may be relevant in the future?</p>

<p>The answer is surprisingly simple. We introduce the concept of experience replay, or memory. Every time we are exposed to a state action pair, we‚Äôll store it away in an special python list type called a <code>deque</code> (it‚Äôs essentially a list of a fixed size, that removes the oldest element each time that you add a new one to it. That way we‚Äôll have a constantly updating buffer of the last <code>n</code> number of state action pairs to train from).</p>

<p>With our experience replay buffer built up, we can randomly sample minibatches of experiences to train from and benefit from a wider look at our environment. Additionally, as our neural net gets better, so do the state action pairs that we train our neural net from. It‚Äôs a win win.</p>

<h2 id="building-the-cartpole-agent">Building the CartPole Agent</h2>

<p>We‚Äôll start by importing our dependencies. Most of it is the same as last time, but we‚Äôll use Keras for our neural net, and matplotlib for plotting our score over time.</p>

<pre><code class="language-python">import numpy as np
import gym
from keras import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import matplotlib.pyplot as plt
from collections import deque
</code></pre>

<p>Next, we‚Äôll build our agent. Note that this is all one class but I‚Äôll try to break it up and talk about each method. Pay particular notice to the indentation here.</p>

<p>Our agent will take in the environment and hold the hyperparameters. We‚Äôll use the <code>env</code> argument to determine our state size and action size.</p>

<pre><code class="language-python">class Agent:
    def __init__(self, env):
        self.memory        = deque(maxlen=600)
        self.state_size    = env.observation_space.shape[0]
        self.action_size   = env.action_space.n
        self.gamma         = 0.95    # discount rate
        self.epsilon       = 1.0     # exploration rate
        self.epsilon_min   = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.model         = self.build_model()
</code></pre>

<p>Notice in the initialisation of our agent, we made a call to a <code>build_model()</code> method. Let‚Äôs write that now to return our neural net from Keras. We‚Äôll store this in a hyperparam so that we can make calls to predict actions or train it later.</p>

<pre><code class="language-python">    def build_model(self):
        model = Sequential()
        model.add(Dense(24, input_dim=self.state_size, activation='relu'))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))
        return model
</code></pre>

<p>Much like our previous tutorial, we‚Äôll need a way to select an action based on our exploration / exploitation trade off. We‚Äôll choose a random number between 1 and 0. If our number is greater than epsilon, we‚Äôll use our neural net to predict which action we should take (exploitation), if it‚Äôs lower, we‚Äôll select and action at random and continue to explore our environment.</p>

<pre><code class="language-python">    def select_action(self, state):
        # Selects an action based on a random number
        # If the number is greater than epsilon, we'll take the predicted action for this state from our neural net
        # If not, we'll choose a random action
        # This helps us navigate the exploration/exploitation trade off
        x = np.random.rand()

        if x &gt; self.epsilon:
            # Exploitation
            actions = self.model.predict(state)
            return np.argmax(actions[0])
        else:
            # Exploration
            return random.randrange(self.action_size)
</code></pre>

<p>Next we‚Äôll introduce the idea of experience replay. We‚Äôll write a very simple function that takes the <code>state</code>, <code>action</code>, <code>reward</code>, <code>next_state</code>, <code>done</code> data returned from taking an action on our environment, and adds it to the end of our deque (removing the oldest element at the same time)‚Ä¶</p>

<pre><code class="language-python">    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
</code></pre>

<p>Lastly, we‚Äôll need a function to train our neural net from our experience replay buffer. Firstly, we‚Äôll make sure that we have enough experiences in our buffer to train from. If we don‚Äôt we‚Äôll simply exit the function and keep exploring our environment until we do.</p>

<p>When we have enough experiences to sample from, we‚Äôll take a random sample of experiences which we‚Äôll call our minibatch, and use that to train the network by calculating our predicted Q-values.</p>

<p>Finally, we‚Äôll reduce our epsilon to gradually nudge us more and more towards exploitation of our neural net in prediction our Q value, rather than exploring our environment by taking random actions.</p>

<pre><code class="language-python">    def train_with_replay(self, batch_size):
        # If we dont have enough experiences to train, we'll exit this function
        if len(self.memory) &lt; batch_size:
            return
        else:
            # Sample a random minibatch of states
            minibatch = random.sample(self.memory, batch_size)

            # For each var in the minibatch, train the network...
            for state, action, reward, next_state, done in minibatch:
                # If we haven't finished the game, calculate our discounted, predicted q value...
                if not done:
                    q_update_target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])
                else:
                    # If we have finished the game, our q-value is our final reward
                    q_update_target = reward

                # Update the predicted q-value for action we tool
                q_values            = self.model.predict(state)
                q_values[0][action] = q_update_target

                # Train model on minibatches from memory
                self.model.fit(state, q_values, epochs=1, verbose=0)

                # Reduce epsilon
                if self.epsilon &gt; self.epsilon_min:
                    self.epsilon *= self.epsilon_decay
</code></pre>

<h2 id="training-our-deep-q-network">Training our Deep Q-Network</h2>

<p>With our agent written, we‚Äôll piece everything together and start training our deep Q-network. We‚Äôll start by defining our cart pole environment and setting our environment specific hyperparameters like number of episodes and minibatch size. We‚Äôll also keep track of our scores in an array in order to graph them out at the end.</p>

<pre><code class="language-python">env        = gym.make('CartPole-v0')
episodes   = 5000
max_steps  = 200
batch_size = 32
agent      = Agent(env)
scores     = []
</code></pre>

<p>We‚Äôll loop through our total number of episodes, and, in a smaller loop, step through our environment, taking actions and observing their rewards. We‚Äôll add our observation to the experience replay buffer. At the end of our game, we‚Äôll print our score, and train our agent on a random minibatch of experiences at the end of each episode.</p>

<pre><code class="language-python">for episode in range(episodes):
    # Reset the environment
    state = env.reset()
    state = np.reshape(state, [1, 4])

    score = 0
    done = False

    for step in range(max_steps):
        # Render the env
        #env.render()

        # Select an action
        action = agent.select_action(state)

        # Take the action and observe our new state
        next_state, reward, done, info = env.step(action)
        next_state = np.reshape(next_state, [1, 4])

        # Add our tuple to memory
        agent.remember(state, action, reward, next_state, done)

        state = next_state
        score += 1

        if done:
            scores.append(score)

            if episode % 500 == 0:
                # print the step as a score and break out of the loop
                # The more steps we did, the better our bot is
                print("episode: {}/{}, score: {}".format(episode, episodes, score))
            break

    agent.train_with_replay(batch_size)
</code></pre>

<h2 id="graphing-our-scores">Graphing our scores</h2>

<p>Finally, we can check how our agent performed over training by printing the score at each episode‚Ä¶</p>

<pre><code class="language-python">y = scores
x = range(len(y))
plt.plot(x, y)
plt.show()
</code></pre>

<p><img src="/assets/images/we_need_to_go_deeper/graph.png" alt="Plot of scores over training time" /></p>

<h2 id="summary">Summary</h2>

<p>We dealt with a larger state space by ditching our Q-table in favour of a neural network to approximate our Q-values of taking a particular action at a particular state. Our agent starts by exploring our space and very quickly learns to maximise its time playing the game. We navigated the problems in training our neural net by taking advantage of an experience replay buffer to stop our agent favouring recent experiences.</p>

<p>Deep Q Networks can be useful for exploring larger state spaces, but they also come with their own trade offs; mainly that we‚Äôre still using a very handy API to explore our environment. In future posts we‚Äôll look at how we can handle more generic game spaces by building agents that can adapt to a wide variety of games.</p>

  </article>

  <div role="alert" class="mt-24 mb-12 alert alert-error alert-dash alert-vertical sm:alert-horizontal">
  üëç
  <div>
    <h3 class="font-bold">Thanks for reading</h3>
    <p class="">You can follow me here for my latest thoughts and projects</p>
  </div>
  <div class="flex items-center gap-4">
    <a href="https://github.com/spencerldixon" target="_blank">
      <i class="fa-brands fa-github fa-xl"></i>
    </a>

    <a href="https://twitter.com/spencerldixon" target="_blank">
      <i class="fa-brands fa-x-twitter fa-xl"></i>
    </a>

    <a href="https://linkedin.com/in/spencerldixon" target="_blank">
      <i class="fa-brands fa-linkedin fa-xl"></i>
    </a>

    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')">
      <i class="fa-solid fa-envelope fa-xl"></i>
    </a>

    <a href="http://localhost:4000/feed.xml">
      <i class="fa-solid fa-rss fa-xl"></i>
    </a>
  </div>
</div>




</div>

    </main>

    <!-- Highlight.js core -->
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.11.1/build/highlight.min.js"></script>
    <script>
    // Auto-detect & highlight all <pre><code class="language-..."> blocks
    document.addEventListener("DOMContentLoaded", (event) => {
      if (window.hljs) { hljs.highlightAll(); }
    });
    </script>
  </body>
</html>
