<!DOCTYPE html>
<html lang="en" data-theme="light">
  <head>
    <meta charset="utf-8">
    <meta content="ie=edge" http-equiv="x-ua-compatible">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-SnH5WK+bZxgPHs44uWIX+LLJAJ9/2PkPKZ5QiAj6Ta86w+fsb2TkcmfRyVX3pBnMFcV7oQPJkl9QevSCWr3W6A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

    <!-- Highlight.js Tokyo Night (dark) from jsDelivr / highlight.js CDN -->
<link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.11.1/build/styles/tokyo-night-dark.min.css">


    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SLLMJ5YC4K"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SLLMJ5YC4K');
</script>

  </head>

  <body class="bg-slate-50 text-base-content min-h-screen p-6">
    <div class="flex justify-between items-center w-full">
  <a href="https://spencerldixon.github.io" class="flex-1">
    <img class="w-12 h-12 rounded-full" src="/assets/images/headshot.jpg" />
  </a>

  <div class="hidden sm:flex w-fit justify-center items-center bg-slate-200 p-2 rounded-full font-semibold">
    <a href="/" class="hover:bg-slate-300 py-2 px-4 rounded-full">Home</a>
    <a href="https://drive.google.com/file/d/10v_i5GDWsbjHuOF4qq_nvTrEcAoZPUfY/view?usp=sharing" target="_blank" class="hover:bg-slate-300 py-2 px-4 rounded-full">Resume</a>
    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')" class="hover:bg-slate-300 py-2 px-4 rounded-full">Contact</a>
  </div>

  <div class="hidden sm:flex gap-4 flex-1 justify-end">
    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')">
      <i class="fa-solid fa-xl fa-envelope"></i>
    </a>


    <a href="https://spencerldixon.github.io/feed.xml">
      <i class="fa-solid fa-xl fa-rss"></i>
    </a>
  </div>

  <label class="block sm:hidden relative z-40 cursor-pointer px-3 py-6 flex-1 flex justify-end" for="mobile-menu">
    <input class="peer hidden" type="checkbox" id="mobile-menu" />
    <div
        class="relative z-50 block h-[2px] w-7 bg-black bg-transparent content-[''] before:absolute before:top-[-0.35rem] before:z-50 before:block before:h-full before:w-full before:bg-black before:transition-all before:duration-200 before:ease-out before:content-[''] after:absolute after:right-0 after:bottom-[-0.35rem] after:block after:h-full after:w-full after:bg-black after:transition-all after:duration-200 after:ease-out after:content-[''] peer-checked:bg-transparent before:peer-checked:top-0 before:peer-checked:w-full before:peer-checked:rotate-45 before:peer-checked:transform after:peer-checked:bottom-0 after:peer-checked:w-full after:peer-checked:-rotate-45 after:peer-checked:transform"
        >
    </div>
      <div
          class="fixed inset-0 z-40 hidden h-full w-full bg-black/50 backdrop-blur-sm peer-checked:block"
          >
          &nbsp;
      </div>
        <div
            class="fixed top-0 right-0 z-40 h-full w-full translate-x-full overflow-y-auto overscroll-y-none transition duration-500 peer-checked:translate-x-0"
            >
            <div class="float-right min-h-full w-[80%] bg-slate-50 px-8 pt-24 shadow-2xl">
              <menu class="flex flex-col gap-4 text-xl">
                <li><a href="/" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Home</a></li>
                <li><a href="https://drive.google.com/file/d/10v_i5GDWsbjHuOF4qq_nvTrEcAoZPUfY/view?usp=sharing" target="_blank" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Resume</a></li>
                <li><a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')" class="flex rounded-full py-2 px-6 hover:bg-slate-200">Contact</a></li>
              </menu>
            </div>
        </div>
  </label>
</div>


    <main class="container mx-auto">
      <div class="max-w-screen-md mx-auto mt-24">
  <div class="flex flex-col gap-2 sm:mt-12 mb-12">
    <h1 class="text-6xl text-primary tracking-tight mb-4">A Primer on Reinforcement Learning: Q-Learning</h1>
    <time class="">01 Nov 2018</time>
  </div>

  <article class="mx-auto prose lg:prose-xl text-base-content">
    <p>Reinforcement Learning is a field of Machine Learning which aims to develop intelligences that learn through trial and error by exploring and interacting with their environment.</p>

<p>You may have seen exciting demos of an AI learning to play video games or robot arms learning to manipulate objects or mimic tasks.</p>

<p>In this post, we‚Äôll look at a very basic approach to Reinforcement Learning which we can use to learn to play very simple games. It‚Äôs a limited approach and we‚Äôll quickly find problems with it, but that will set us up nicely for a follow up post on how we can improve on this technique</p>

<h2 id="state-actions-and-rewards">State, Actions and Rewards</h2>

<p>Reinforcement Learning (RL) consists of two actors: an agent (our model / algorithm), and the environment (the game we‚Äôre playing in this case). Our agent seeks to develop an optimal policy for interacting with the environment that maximises the cumulative reward over time.</p>

<p>Our environment is represented by a series of <em>states</em>. If we think of a grid with a single playing piece, we can transition states by moving our piece around the board. Each move takes us to a different possible state that the game can be in. We have several different things we can do to our piece, known as <em>actions</em>. We can move up, down, left, or right. Each of these actions transitions us to a new state and brings us one step closer to, or further away from, our goal state (usually a state that will win the game).</p>

<p>When we take an action in our game, we receive some feedback, usually in the form of a score. It‚Äôs this <em>reward</em> that we seek to maximise over our time playing the game.</p>

<p>In short, our agent interacts with our environment by choosing <em>actions</em> (a) to take at a particular <em>state</em> (s) with the intention of maximising future <em>rewards</em> (r) received. It‚Äôs this mapping of what action we should take at what particular state that‚Äôs the problem that we need to solve and is referred to as the <em>policy</em> (œÄ), with the optimal policy being denoted as <em>œÄ&amp;ast;</em></p>

<p><img src="/assets/images/a_primer_on_reinforcement_learning/cycle.png" alt="The Reinforcement Learning Cycle" /></p>

<h2 id="the-q-table">The Q-Table</h2>

<p>There are a lot of different techniques that we can use to get our model to converge on an optimal policy but in this post, we‚Äôre going to go for the one which allows us to get something up and running quickly.</p>

<p>Q-Learning is a technique that seeks to assess the <em>quality</em> of taking a given action at a given state. If we drew up a table, with all our game states as rows, and all our possible actions as columns. We could use it as a cheat sheet to keep track of the rewards we recieve for a state action pair over time. If we move left at state 1, and receive a reward of +1, we‚Äôll write that down in our table. Over time, we‚Äôll begin to build up a picture of the quality of every action we can take at every state, and could easily select the actions with the biggest score, or Q-value, to cheat our way to the end of the game.</p>

<p>But this raises two important questions, how do we update our score for each state action pair? And, how do we move around our game when we have no values in our table yet?</p>

<h2 id="the-bellman-equation">The Bellman Equation</h2>

<p>The Bellman Equation provides the foundation for assessing the quality of taking a given action at a given state. If we know the rewards at each state in the game, for example, landing on a safe tile is +1 point, and losing the game is -10 points, we can use the Bellman equation to calculate the optimal Q-value for each state action pair.</p>

<p>We‚Äôll iteratively update our Q-values in our Q-table until we converge to the optimal policy, seeking to reduce the loss between our Q-value, and our optimal Q-value.</p>

<p>Firstly, we‚Äôll need to take an action from our state. Then we‚Äôll receive our reward, along with a new state. We can use this new information, along with the information of the action we took at the previous state to assess how good or bad our move was.</p>

<p>What if we come across a reward for a state action pair we‚Äôve already seen? Over writing previous Q-values would lose valuable information about previous game plays. Instead, we can use a learning rate to update our Q-value. The higher the learning rate, the more quickly our agent will adopt new values and disregard previous values. With a learning rate of 1.0, our agent would simply rewrite all the old values with new values each time.</p>

<p><img src="/assets/images/a_primer_on_reinforcement_learning/bellman.png" alt="The Bellman Equation" /></p>

<h2 id="exploration-vs-exploitation">Exploration vs Exploitation</h2>

<p>Imagine our agent is playing a game with an empty Q table. Initially, it‚Äôll be fairly useless, so using it as a policy to follow won‚Äôt get us anywhere. In fact, when we do build up values in our Q table, we‚Äôll want to avoid using them too early before they‚Äôve converged. If we exploit our table too soon, we‚Äôll get stuck at early rewards and possibly miss out on taking larger rewards that could benefit us long term.</p>

<p>Instead, we‚Äôll want to strike a decaying balance between how much we want to explore the games state spaces to find new rewards, and how much we want to exploit our Q table for the correct answers. This is referred to as exploration vs exploitation.</p>

<p>We‚Äôll do this with a strategy known as <em>epsilon greedy</em>. We‚Äôll set an <code>epsilon</code> number, which represents our probability of choosing exploration vs exploitation. With epsilon set to 1.0, we have a 100% chance that we‚Äôll explore our environment, and thus take actions entirely at random. With epsilon at 0, we‚Äôll exploit our Q-table and select the action with the highest value. As we iterate through our training, we‚Äôll slowly decay epsilon by <code>gamma</code>, a small number, that will make it less and less probable over time that we‚Äôll explore our environment by selecting random actions.</p>

<h2 id="frozen-lake">Frozen Lake</h2>

<p>Frozen Lake is a game where we have to navigate a 4x4 grid of tiles, each of a different surface type. S is our starting point, G is our goal point, F are frozen tiles (safe to step on) and H are holes which we can fall into and lose the game. We‚Äôll train an agent to safely navigate from the starting tile to our goal tile using Q-Learning.</p>

<pre><code class="language-python">SFFF
FHFH
FFFH
HFFG
</code></pre>

<p>We‚Äôll first import OpenAI‚Äôs gym library, which will give us the FrozenLake game, with a nice wrapper to be able to access actions and the state space. We‚Äôll also require numpy and random.</p>

<pre><code class="language-python">import numpy as np
import random
import gym
</code></pre>

<p>Next we‚Äôll start up our FrozenLake game and assign the environment to a variable we can use later on</p>

<pre><code class="language-python">env = gym.make("FrozenLake-v0")
</code></pre>

<p>In order to form our Q table, we‚Äôll need to know the number of possible actions and states. We‚Äôll then create an empty table, initialised with zeros at the moment, that we can later update throughout our training with our Q values, much like how we update weights in a neural network. Our Q table acts like a cheat sheet, reflecting the quality of taking that particular action, at a particular state.</p>

<pre><code class="language-python">action_size = env.action_space.n
state_size = env.observation_space.n

qtable = np.zeros((state_size, action_size))
print(qtable)
</code></pre>

<pre><code class="language-python">[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
</code></pre>

<p>Let‚Äôs set some hyperparameters‚Ä¶</p>

<pre><code class="language-python">episodes       = 10000      # Total episodes
max_steps      = 99         # Max moves per episode - stops us exploring infinitely

learning_rate  = 0.8        # Learning rate
gamma          = 0.95       # Discounting rate

epsilon        = 1.0        # Exploration vs exploitation rate
decay_rate     = 0.001      # How much we want to decay our exploration vs exploitation rate
</code></pre>

<p>We‚Äôll write some helper functions that will make it easier to understand what our code is doing without getting caught up in the formulas.</p>

<p>Firstly, we‚Äôll need a function that, over time, will make a gradual progression from exploration of our environment, to exploitation by utilising our Q table. We‚Äôll use epsilon to denote our exploration vs exploitation rate, and reduce it by our <code>decay_rate</code> for every episode.</p>

<p><code>max_epsilon</code> is the largest our epsilon can be and represents a full 100% chance we‚Äôll explore our environment. Conversely, <code>min_epsilon</code> represents a 100% chance that we‚Äôll exploit our Q table for the correct answers.</p>

<pre><code class="language-python">def reduce_epsilon(episode, min_epsilon=0.01, max_epsilon=1.0, decay_rate=0.001):
    return min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)
</code></pre>

<p>Epsilon will be used in selecting wether we want to explore our environment, which we‚Äôll do by selecting an action at random, or wether we want to exploit our learned Q-table, which we can do by selecting the action with the highest Q value.</p>

<p>Since this is fairly simply logic, we can code this into another helper function which will help simplify our code‚Ä¶</p>

<p>Here we take in some information like our <code>epsilon</code>, our <code>qtable</code>, <code>state</code> and the <code>env</code> (environment) and generate a random number. If our number is larger than epsilon, we‚Äôll choose to exploit our Q-table by selecting the action with the highest Q-value. If our number is lower than epsilon, we‚Äôll explore our environment further by selecting a random action from our action space.</p>

<pre><code class="language-python">def select_action(epsilon, qtable, state, env):
    x = random.uniform(0,1)

    if x &gt; epsilon:
        # Exploitation
        return np.argmax(qtable[state,:])
    else:
        # Exploration
        return env.action_space.sample()
</code></pre>

<p>Lastly, we‚Äôll need a function to update the values in our Q-table based upon the Bellman equation given our previous state, action taken, reward, and new state‚Ä¶</p>

<pre><code class="language-python">def update_qtable(qtable, state, action, reward, new_state, learning_rate, gamma):
    # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
    # qtable[new_state,:] : all the actions we can take from new state

    qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])
    return qtable
</code></pre>

<h2 id="training-our-q-table">Training our Q-Table</h2>

<p>Next up is the bulk of our code. This is where we‚Äôll put the pieces together, train our agent, and populate our Q-table.</p>

<p>For every episode in our total number of <code>episodes</code>, we‚Äôll firstly reset our environment and a few variables which will keep track of game play for that particular run. For each step, we‚Äôll use our <code>select_action()</code> function to choose an action, either at random (exploration) or from our Q-table (exploitation). This rate will gradually ramp towards more and more exploitation over time as we build up our Q-table.</p>

<p>We‚Äôll then take our action and observe the reward and new state returned, which we‚Äôll use to update our Q-table. Finally, we‚Äôll we‚Äôll set our <code>state</code> to be the <code>new_state</code> that we received by taking an action, reduce <code>epsilon</code> to lean slightly more towards exploitation, add our reward to a list so that we can keep track of how we‚Äôre improving over time, and start the cycle over again until we reach some terminal state in our game (we fall into a hole, or win the game).</p>

<pre><code class="language-python">rewards = []

for episode in range(episodes):
    # Reset the environment
    state = env.reset()
    step = 0
    done = False
    total_rewards = 0

    for step in range(max_steps):
        # Use epsilon to pick an action, either at random, or from our q-table
        action = select_action(epsilon, qtable, state, env)

        # Take the action and observe the new state and reward
        new_state, reward, done, info = env.step(action)

        # Update our Q-table to take note of how valuable the action according to the reward we got
        qtable = update_qtable(qtable, state, action, reward, new_state, learning_rate, gamma)

        # Set state to the new state we received (where we moved to)
        state = new_state

        total_rewards += reward

        # If the game is over, exit the loop, back to a new training loop
        if done == True:
            break

        epsilon = reduce_epsilon(episode)

    rewards.append(total_rewards)

print("Score over time: " +  str(sum(rewards)/total_episodes))
print(qtable)
</code></pre>

<h2 id="playing-the-game">Playing the Game</h2>

<p>Once we‚Äôve populated our Q-table, we can exploit it to play the game successfully. As we‚Äôre simply following our Q-table, we no longer have to deal with updating our table, or dealing with our exploration vs exploitation trade off. We can simply just follow the policy of selecting the highest value at a given state. With a well trained Q-table, our values should closely reflect the maximum expected reward over time by taking that particular action at that particular state.</p>

<pre><code class="language-python">env.reset()

for episode in range(5):
    state = env.reset()
    step = 0
    done = False
    print("Playing Round #", episode)

    for step in range(max_steps):
        # Select the action with the highest reward
        action = np.argmax(qtable[state,:])

        # Return our new state and reward
        new_state, reward, done, info = env.step(action)

        if done:
            # If the game is finished, we'll print our environment to see if we fell into a hole, or ended on our goal tile
            env.render()

            # We print the number of steps it took.
            print("Steps taken:", step)
            break

        state = new_state

env.close()
</code></pre>

<h2 id="summary">Summary</h2>

<p>And there we have it! We successfully trained a model to learn play the Frozen Lake game by exploring the environment itself, and learning through trial and error.</p>

<p>But what happens when we want to play a more complex game with millions of possible states? Unfortunately, as your state space grows, you very quickly out grow the feasibility of using a Q-table. It would take millions of iterations to even begin to explore all the possible state spaces and build up an accurate Q-table.</p>

<p>Instead of creating a cheat sheet that we can look up every possible value for every possible action in every possible state, what if we could just simplify by having a function that approximates the Q-value for a given state action pair?</p>

<p>If you have read previous posts, you may be familiar with one tool that we can use for function approximation; the neural network.</p>

<p>In a future post we‚Äôll look at how we can improve on our reinforcement learning agent by using neural networks to apply our techniques to larger state spaces and more complex games with Deep Q-Learning.</p>

  </article>

  <div role="alert" class="mt-24 mb-12 alert alert-error alert-dash alert-vertical sm:alert-horizontal">
  üëç
  <div>
    <h3 class="font-bold">Thanks for reading</h3>
    <p class="">You can follow me here for my latest thoughts and projects</p>
  </div>
  <div class="flex items-center gap-4">
    <a href="https://github.com/spencerldixon" target="_blank">
      <i class="fa-brands fa-github fa-xl"></i>
    </a>

    <a href="https://twitter.com/spencerldixon" target="_blank">
      <i class="fa-brands fa-x-twitter fa-xl"></i>
    </a>

    <a href="https://linkedin.com/in/spencerldixon" target="_blank">
      <i class="fa-brands fa-linkedin fa-xl"></i>
    </a>

    <a href="javascript:window.location.href='mailto:' + atob('c3BlbmNlcmxsb3lkZGl4b25AZ21haWwuY29t')">
      <i class="fa-solid fa-envelope fa-xl"></i>
    </a>

    <a href="https://spencerldixon.github.io/feed.xml">
      <i class="fa-solid fa-rss fa-xl"></i>
    </a>
  </div>
</div>




</div>

    </main>

    <!-- Highlight.js core -->
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.11.1/build/highlight.min.js"></script>
    <script>
    // Auto-detect & highlight all <pre><code class="language-..."> blocks
    document.addEventListener("DOMContentLoaded", (event) => {
      if (window.hljs) { hljs.highlightAll(); }
    });
    </script>
  </body>
</html>
